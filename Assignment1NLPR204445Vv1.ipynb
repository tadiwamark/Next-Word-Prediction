{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df24ab92",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "3b460db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tadiw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.collocations import *\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670afb48",
   "metadata": {},
   "source": [
    "### Loading the Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "640b15f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f=open('the-great-gatsby.txt')\n",
    "script=[]\n",
    "script=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "bc049aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Great',\n",
       " 'Gatsby',\n",
       " 'By',\n",
       " 'F.',\n",
       " 'Scott',\n",
       " 'Fitzgerald',\n",
       " '.',\n",
       " 'Then',\n",
       " 'wear']"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "book = word_tokenize(script)\n",
    "book[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "1788d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english') + list(string.punctuation) + ['“','”','...',\"''\",'’','``', \"'\", \"‘\"]\n",
    "custom_stopwords = ['–', 'also', 'something', 'cf', 'thus', 'two', 'now', 'would', 'make', 'eb', 'u', 'well', 'even', 'said', 'eg', 'us',\n",
    "                    'n', 'sein', 'e', 'da', 'therefore', 'however', 'would', 'thing', 'must', 'merely', 'way', 'since', 'latter', 'first',\n",
    "                    'B', 'mean', 'upon', 'yet', 'cannot', 'c', 'C', 'let', 'may', 'might', \"'s\", 'b', 'ofthe', 'p.', '_', '-', 'eg', 'e.g.',\n",
    "                    'ie', 'i.e.', 'f', 'l', \"n't\", 'e.g', 'i.e', '—', '--', 'hyl', 'phil', 'one', 'press', 'cent', 'place'] + stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "150e3459",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_words = [x.lower() for x in book if x.lower() not in custom_stopwords]\n",
    "freq_dist = FreqDist(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "c7391085",
   "metadata": {
    "executionInfo": {
     "elapsed": 239660,
     "status": "ok",
     "timestamp": 1610851172763,
     "user": {
      "displayName": "Kourosh Alizadeh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjBukpyAH378LODkjvweVlXJV9CdOdTH4RUedr6FQ=s64",
      "userId": "01534718482150837193"
     },
     "user_tz": 420
    },
    "id": "RPWXAucGD9pT"
   },
   "outputs": [],
   "source": [
    "uninformative_words = ['else', 'shall', 'either', 'still', 'rather', 'another', 'made', 'without']\n",
    "book_stopwords = custom_stopwords + [x[0] for x in freq_dist.most_common(50)] + uninformative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "4573c392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Great',\n",
       " 'Gatsby',\n",
       " 'By',\n",
       " 'F.',\n",
       " 'Scott',\n",
       " 'Fitzgerald',\n",
       " '.',\n",
       " 'Then',\n",
       " 'wear',\n",
       " 'the',\n",
       " 'gold',\n",
       " 'hat',\n",
       " ',',\n",
       " 'if',\n",
       " 'that',\n",
       " 'will',\n",
       " 'move',\n",
       " 'her',\n",
       " ';',\n",
       " 'If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'bounce',\n",
       " 'high',\n",
       " ',',\n",
       " 'bounce',\n",
       " 'for',\n",
       " 'her',\n",
       " 'too',\n",
       " ',',\n",
       " 'Till',\n",
       " 'she',\n",
       " 'cry',\n",
       " 'â€˜Lover',\n",
       " ',',\n",
       " 'gold-hatted',\n",
       " ',',\n",
       " 'high-bouncing',\n",
       " 'lover',\n",
       " ',',\n",
       " 'I',\n",
       " 'must',\n",
       " 'have',\n",
       " 'you',\n",
       " '!',\n",
       " 'â€™',\n",
       " 'â€',\n",
       " '”',\n",
       " 'THOMAS',\n",
       " 'PARKE',\n",
       " 'Dâ€™INVILLIERS',\n",
       " '\\x18',\n",
       " 'The',\n",
       " 'Great',\n",
       " 'Gatsby',\n",
       " 'Chapter',\n",
       " '1',\n",
       " 'I',\n",
       " 'n',\n",
       " 'my',\n",
       " 'younger',\n",
       " 'and',\n",
       " 'more',\n",
       " 'vulnerable',\n",
       " 'years',\n",
       " 'my',\n",
       " 'father',\n",
       " 'gave',\n",
       " 'me',\n",
       " 'some',\n",
       " 'advice',\n",
       " 'that',\n",
       " 'Iâ€™ve',\n",
       " 'been',\n",
       " 'turning',\n",
       " 'over',\n",
       " 'in',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'ever',\n",
       " 'since',\n",
       " '.',\n",
       " 'â€˜Whenever',\n",
       " 'you',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'criticizing',\n",
       " 'any',\n",
       " 'one',\n",
       " ',',\n",
       " 'â€™',\n",
       " 'he',\n",
       " 'told',\n",
       " 'me',\n",
       " ',',\n",
       " 'â€˜just',\n",
       " 'remember',\n",
       " 'that',\n",
       " 'all',\n",
       " 'the',\n",
       " 'people',\n",
       " 'in',\n",
       " 'this',\n",
       " 'world',\n",
       " 'havenâ€™t',\n",
       " 'had',\n",
       " 'the',\n",
       " 'advantages',\n",
       " 'that',\n",
       " 'youâ€™ve',\n",
       " 'had.â€™',\n",
       " 'He',\n",
       " 'didnâ€™t',\n",
       " 'say',\n",
       " 'any',\n",
       " 'more',\n",
       " 'but',\n",
       " 'weâ€™ve',\n",
       " 'always',\n",
       " 'been',\n",
       " 'unusually',\n",
       " 'communicative',\n",
       " 'in',\n",
       " 'a',\n",
       " 'reserved',\n",
       " 'way',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'understood',\n",
       " 'that',\n",
       " 'he',\n",
       " 'meant',\n",
       " 'a',\n",
       " 'great',\n",
       " 'deal',\n",
       " 'more',\n",
       " 'than',\n",
       " 'that',\n",
       " '.',\n",
       " 'In',\n",
       " 'consequence',\n",
       " 'Iâ€™m',\n",
       " 'inclined',\n",
       " 'to',\n",
       " 'reserve',\n",
       " 'all',\n",
       " 'judgments',\n",
       " ',',\n",
       " 'a',\n",
       " 'habit',\n",
       " 'that',\n",
       " 'has',\n",
       " 'opened',\n",
       " 'up',\n",
       " 'many',\n",
       " 'curious',\n",
       " 'natures',\n",
       " 'to',\n",
       " 'me',\n",
       " 'and',\n",
       " 'also',\n",
       " 'made',\n",
       " 'me',\n",
       " 'the',\n",
       " 'victim',\n",
       " 'of',\n",
       " 'not',\n",
       " 'a',\n",
       " 'few',\n",
       " 'veteran',\n",
       " 'bores',\n",
       " '.',\n",
       " 'The',\n",
       " 'abnormal',\n",
       " 'mind',\n",
       " 'is',\n",
       " 'quick',\n",
       " 'to',\n",
       " 'detect',\n",
       " 'and',\n",
       " 'attach',\n",
       " 'itself',\n",
       " 'to',\n",
       " 'this',\n",
       " 'quality',\n",
       " 'when',\n",
       " 'it',\n",
       " 'appears',\n",
       " 'in',\n",
       " 'a',\n",
       " 'normal',\n",
       " 'person',\n",
       " ',',\n",
       " 'and',\n",
       " 'so',\n",
       " 'it',\n",
       " 'came',\n",
       " 'about',\n",
       " 'that',\n",
       " 'in',\n",
       " 'college',\n",
       " 'I',\n",
       " 'was',\n",
       " 'unjustly',\n",
       " 'accused',\n",
       " 'of',\n",
       " 'being',\n",
       " 'a',\n",
       " 'politician',\n",
       " ',',\n",
       " 'because',\n",
       " 'I',\n",
       " 'was',\n",
       " 'privy',\n",
       " 'to',\n",
       " 'the',\n",
       " 'secret',\n",
       " 'griefs',\n",
       " 'of',\n",
       " 'wild',\n",
       " ',',\n",
       " 'unknown',\n",
       " 'men',\n",
       " '.',\n",
       " 'Most',\n",
       " 'of',\n",
       " 'the',\n",
       " 'confidences',\n",
       " 'were',\n",
       " 'unsoughtâ€',\n",
       " '”',\n",
       " 'frequently',\n",
       " 'I',\n",
       " 'have',\n",
       " 'feigned',\n",
       " 'sleep',\n",
       " ',',\n",
       " 'preoccupation',\n",
       " ',',\n",
       " 'or',\n",
       " 'a',\n",
       " 'hostile',\n",
       " 'levity',\n",
       " 'when',\n",
       " 'I',\n",
       " 'realized',\n",
       " 'by',\n",
       " 'some',\n",
       " 'unmistakable',\n",
       " 'sign',\n",
       " 'that',\n",
       " 'an',\n",
       " 'intimate',\n",
       " 'revelation',\n",
       " 'was',\n",
       " 'quivering',\n",
       " 'on',\n",
       " 'the',\n",
       " 'horizonâ€',\n",
       " '”',\n",
       " 'for',\n",
       " 'the',\n",
       " 'intimate',\n",
       " 'revelations',\n",
       " 'of',\n",
       " 'young',\n",
       " 'men',\n",
       " 'or',\n",
       " 'at',\n",
       " 'least',\n",
       " 'the',\n",
       " 'terms',\n",
       " 'in',\n",
       " 'which',\n",
       " 'they',\n",
       " 'express',\n",
       " 'them',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'plagiaristic',\n",
       " 'and',\n",
       " 'marred',\n",
       " 'by',\n",
       " 'obvious',\n",
       " 'suppressions',\n",
       " '.',\n",
       " 'Reserving',\n",
       " 'judgments',\n",
       " 'is',\n",
       " 'a',\n",
       " 'matter',\n",
       " 'of',\n",
       " 'infinite',\n",
       " 'hope',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'still',\n",
       " 'a',\n",
       " 'little',\n",
       " 'afraid',\n",
       " 'of',\n",
       " 'missing',\n",
       " 'something',\n",
       " 'if',\n",
       " 'I',\n",
       " 'forget',\n",
       " 'that',\n",
       " ',',\n",
       " 'as',\n",
       " 'my',\n",
       " 'fa',\n",
       " '\\x18',\n",
       " 'ther',\n",
       " 'snobbishly',\n",
       " 'suggested',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'snobbishly',\n",
       " 'repeat',\n",
       " 'a',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'decencies',\n",
       " 'is',\n",
       " 'parcelled',\n",
       " 'out',\n",
       " 'unequally',\n",
       " 'at',\n",
       " 'birth',\n",
       " '.',\n",
       " 'And',\n",
       " ',',\n",
       " 'after',\n",
       " 'boasting',\n",
       " 'this',\n",
       " 'way',\n",
       " 'of',\n",
       " 'my',\n",
       " 'tolerance',\n",
       " ',',\n",
       " 'I',\n",
       " 'come',\n",
       " 'to',\n",
       " 'the',\n",
       " 'admission',\n",
       " 'that',\n",
       " 'it',\n",
       " 'has',\n",
       " 'a',\n",
       " 'limit',\n",
       " '.',\n",
       " 'Conduct',\n",
       " 'may',\n",
       " 'be',\n",
       " 'founded',\n",
       " 'on',\n",
       " 'the',\n",
       " 'hard',\n",
       " 'rock',\n",
       " 'or',\n",
       " 'the',\n",
       " 'wet',\n",
       " 'marshes',\n",
       " 'but',\n",
       " 'after',\n",
       " 'a',\n",
       " 'certain',\n",
       " 'point',\n",
       " 'I',\n",
       " 'donâ€™t',\n",
       " 'care',\n",
       " 'what',\n",
       " 'itâ€™s',\n",
       " 'founded',\n",
       " 'on',\n",
       " '.',\n",
       " 'When',\n",
       " 'I',\n",
       " 'came',\n",
       " 'back',\n",
       " 'from',\n",
       " 'the',\n",
       " 'East',\n",
       " 'last',\n",
       " 'autumn',\n",
       " 'I',\n",
       " 'felt',\n",
       " 'that',\n",
       " 'I',\n",
       " 'wanted',\n",
       " 'the',\n",
       " 'world',\n",
       " 'to',\n",
       " 'be',\n",
       " 'in',\n",
       " 'uniform',\n",
       " 'and',\n",
       " 'at',\n",
       " 'a',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'moral',\n",
       " 'attention',\n",
       " 'forever',\n",
       " ';',\n",
       " 'I',\n",
       " 'wanted',\n",
       " 'no',\n",
       " 'more',\n",
       " 'riotous',\n",
       " 'excursions',\n",
       " 'with',\n",
       " 'privileged',\n",
       " 'glimpses',\n",
       " 'into',\n",
       " 'the',\n",
       " 'human',\n",
       " 'heart',\n",
       " '.',\n",
       " 'Only',\n",
       " 'Gatsby',\n",
       " ',',\n",
       " 'the',\n",
       " 'man',\n",
       " 'who',\n",
       " 'gives',\n",
       " 'his',\n",
       " 'name',\n",
       " 'to',\n",
       " 'this',\n",
       " 'book',\n",
       " ',',\n",
       " 'was',\n",
       " 'exempt',\n",
       " 'from',\n",
       " 'my',\n",
       " 'reactionâ€',\n",
       " '”',\n",
       " 'Gatsby',\n",
       " 'who',\n",
       " 'represented',\n",
       " 'everything',\n",
       " 'for',\n",
       " 'which',\n",
       " 'I',\n",
       " 'have',\n",
       " 'an',\n",
       " 'unaffected',\n",
       " 'scorn',\n",
       " '.',\n",
       " 'If',\n",
       " 'personality',\n",
       " 'is',\n",
       " 'an',\n",
       " 'unbroken',\n",
       " 'series',\n",
       " 'of',\n",
       " 'successful',\n",
       " 'gestures',\n",
       " ',',\n",
       " 'then',\n",
       " 'there',\n",
       " 'was',\n",
       " 'something',\n",
       " 'gorgeous',\n",
       " 'about',\n",
       " 'him',\n",
       " ',',\n",
       " 'some',\n",
       " 'heightened',\n",
       " 'sensitivity',\n",
       " 'to',\n",
       " 'the',\n",
       " 'promises',\n",
       " 'of',\n",
       " 'life',\n",
       " ',',\n",
       " 'as',\n",
       " 'if',\n",
       " 'he',\n",
       " 'were',\n",
       " 'related',\n",
       " 'to',\n",
       " 'one',\n",
       " 'of',\n",
       " 'those',\n",
       " 'intricate',\n",
       " 'machines',\n",
       " 'that',\n",
       " 'register',\n",
       " 'earthquakes',\n",
       " 'ten',\n",
       " 'thousand',\n",
       " 'miles',\n",
       " 'away',\n",
       " '.',\n",
       " 'This',\n",
       " 'responsiveness',\n",
       " 'had',\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'that',\n",
       " 'flabby',\n",
       " 'impressionability',\n",
       " 'which',\n",
       " 'is',\n",
       " 'dignified',\n",
       " 'under',\n",
       " 'the',\n",
       " 'name',\n",
       " 'of',\n",
       " 'the',\n",
       " 'â€˜creative',\n",
       " 'temperamentâ€™â€',\n",
       " '”',\n",
       " 'it',\n",
       " 'was',\n",
       " 'an',\n",
       " 'extraordinary',\n",
       " 'gift',\n",
       " 'for',\n",
       " 'hope',\n",
       " ',',\n",
       " 'a',\n",
       " 'romantic',\n",
       " 'readiness',\n",
       " 'such',\n",
       " 'as',\n",
       " 'I',\n",
       " 'have',\n",
       " 'never',\n",
       " 'found',\n",
       " 'in',\n",
       " 'any',\n",
       " 'other',\n",
       " 'person',\n",
       " 'and',\n",
       " 'which',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'likely',\n",
       " 'I',\n",
       " 'shall',\n",
       " 'ever',\n",
       " 'find',\n",
       " 'again',\n",
       " '.',\n",
       " 'Noâ€',\n",
       " '”',\n",
       " 'Gatsby',\n",
       " 'turned',\n",
       " 'out',\n",
       " 'all',\n",
       " 'right',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " ';',\n",
       " 'it',\n",
       " 'is',\n",
       " 'what',\n",
       " 'preyed',\n",
       " 'on',\n",
       " 'Gatsby',\n",
       " ',',\n",
       " 'what',\n",
       " 'foul',\n",
       " 'dust',\n",
       " 'floated',\n",
       " 'in',\n",
       " 'the',\n",
       " 'wake',\n",
       " 'of',\n",
       " 'his',\n",
       " 'dreams',\n",
       " 'that',\n",
       " 'temporarily',\n",
       " 'closed',\n",
       " 'out',\n",
       " 'my',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'the',\n",
       " 'abortive',\n",
       " 'sorrows',\n",
       " 'and',\n",
       " 'shortwinded',\n",
       " 'elations',\n",
       " 'of',\n",
       " 'men',\n",
       " '.',\n",
       " 'My',\n",
       " 'family',\n",
       " 'have',\n",
       " 'been',\n",
       " 'prominent',\n",
       " ',',\n",
       " 'well-to-do',\n",
       " 'people',\n",
       " 'in',\n",
       " 'this',\n",
       " 'middle-western',\n",
       " 'city',\n",
       " 'for',\n",
       " 'three',\n",
       " 'generations',\n",
       " '.',\n",
       " 'The',\n",
       " 'Car\\x18',\n",
       " 'raways',\n",
       " 'are',\n",
       " 'something',\n",
       " 'of',\n",
       " 'a',\n",
       " 'clan',\n",
       " 'and',\n",
       " 'we',\n",
       " 'have',\n",
       " 'a',\n",
       " 'tradition',\n",
       " 'that',\n",
       " 'weâ€™re',\n",
       " 'descended',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Dukes',\n",
       " 'of',\n",
       " 'Buccleuch',\n",
       " ',',\n",
       " 'but',\n",
       " 'the',\n",
       " 'actual',\n",
       " 'founder',\n",
       " 'of',\n",
       " 'my',\n",
       " 'line',\n",
       " 'was',\n",
       " 'my',\n",
       " 'grandfatherâ€™s',\n",
       " 'brother',\n",
       " 'who',\n",
       " 'came',\n",
       " 'here',\n",
       " 'in',\n",
       " 'fifty-one',\n",
       " ',',\n",
       " 'sent',\n",
       " 'a',\n",
       " 'substitute',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Civil',\n",
       " 'War',\n",
       " 'and',\n",
       " 'started',\n",
       " 'the',\n",
       " 'wholesale',\n",
       " 'hardware',\n",
       " 'business',\n",
       " 'that',\n",
       " 'my',\n",
       " 'father',\n",
       " 'carries',\n",
       " 'on',\n",
       " 'today',\n",
       " '.',\n",
       " 'I',\n",
       " 'never',\n",
       " 'saw',\n",
       " 'this',\n",
       " 'great-uncle',\n",
       " 'but',\n",
       " 'Iâ€™m',\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'look',\n",
       " 'like',\n",
       " 'himâ€',\n",
       " '”',\n",
       " 'with',\n",
       " 'special',\n",
       " 'reference',\n",
       " 'to',\n",
       " 'the',\n",
       " 'rather',\n",
       " 'hard-boiled',\n",
       " 'painting',\n",
       " 'that',\n",
       " 'hangs',\n",
       " 'in',\n",
       " 'Fatherâ€™s',\n",
       " 'office',\n",
       " '.',\n",
       " 'I',\n",
       " 'graduated',\n",
       " 'from',\n",
       " 'New',\n",
       " 'Haven',\n",
       " 'in',\n",
       " '1915',\n",
       " ',',\n",
       " 'just',\n",
       " 'a',\n",
       " 'quarter',\n",
       " 'of',\n",
       " 'a',\n",
       " 'century',\n",
       " 'after',\n",
       " 'my',\n",
       " 'father',\n",
       " ',',\n",
       " 'and',\n",
       " 'a',\n",
       " 'little',\n",
       " 'later',\n",
       " 'I',\n",
       " 'participated',\n",
       " 'in',\n",
       " 'that',\n",
       " 'delayed',\n",
       " 'Teutonic',\n",
       " 'migration',\n",
       " 'known',\n",
       " 'as',\n",
       " 'the',\n",
       " 'Great',\n",
       " 'War',\n",
       " '.',\n",
       " 'I',\n",
       " 'enjoyed',\n",
       " 'the',\n",
       " 'counter-raid',\n",
       " 'so',\n",
       " 'thoroughly',\n",
       " 'that',\n",
       " 'I',\n",
       " 'came',\n",
       " 'back',\n",
       " 'restless',\n",
       " '.',\n",
       " 'Instead',\n",
       " 'of',\n",
       " 'being',\n",
       " 'the',\n",
       " 'warm',\n",
       " 'center',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'the',\n",
       " 'middle-west',\n",
       " 'now',\n",
       " 'seemed',\n",
       " 'like',\n",
       " 'the',\n",
       " 'ragged',\n",
       " 'edge',\n",
       " 'of',\n",
       " 'the',\n",
       " 'universeâ€',\n",
       " '”',\n",
       " 'so',\n",
       " 'I',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'go',\n",
       " 'east',\n",
       " 'and',\n",
       " 'learn',\n",
       " 'the',\n",
       " 'bond',\n",
       " 'business',\n",
       " '.',\n",
       " 'Everybody',\n",
       " 'I',\n",
       " 'knew',\n",
       " 'was',\n",
       " 'in',\n",
       " 'the',\n",
       " 'bond',\n",
       " 'business',\n",
       " 'so',\n",
       " 'I',\n",
       " 'supposed',\n",
       " 'it',\n",
       " 'could',\n",
       " 'support',\n",
       " 'one',\n",
       " 'more',\n",
       " 'single',\n",
       " 'man',\n",
       " '.',\n",
       " 'All',\n",
       " 'my',\n",
       " 'aunts',\n",
       " 'and',\n",
       " 'uncles',\n",
       " 'talked',\n",
       " 'it',\n",
       " 'over',\n",
       " 'as',\n",
       " 'if',\n",
       " 'they',\n",
       " 'were',\n",
       " 'choosing',\n",
       " 'a',\n",
       " 'prep-school',\n",
       " 'for',\n",
       " 'me',\n",
       " 'and',\n",
       " 'finally',\n",
       " 'said',\n",
       " ',',\n",
       " 'â€˜Whyâ€',\n",
       " '”',\n",
       " 'yeesâ€™',\n",
       " 'with',\n",
       " 'very',\n",
       " 'grave',\n",
       " ',',\n",
       " 'hesitant',\n",
       " 'faces',\n",
       " '.',\n",
       " 'Father',\n",
       " 'agreed',\n",
       " 'to',\n",
       " 'finance',\n",
       " 'me',\n",
       " 'for',\n",
       " 'a',\n",
       " 'year',\n",
       " 'and',\n",
       " 'after',\n",
       " 'various',\n",
       " 'delays',\n",
       " 'I',\n",
       " 'came',\n",
       " 'east',\n",
       " ',',\n",
       " 'permanently',\n",
       " ',',\n",
       " 'I',\n",
       " 'thought',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'spring',\n",
       " 'of',\n",
       " 'twenty-two',\n",
       " '.',\n",
       " 'The',\n",
       " 'practical',\n",
       " 'thing',\n",
       " 'was',\n",
       " 'to',\n",
       " 'find',\n",
       " 'rooms',\n",
       " 'in',\n",
       " 'the',\n",
       " 'city',\n",
       " 'but',\n",
       " 'it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'warm',\n",
       " 'season',\n",
       " 'and',\n",
       " 'I',\n",
       " 'had',\n",
       " 'just',\n",
       " 'left',\n",
       " 'a',\n",
       " 'country',\n",
       " 'of',\n",
       " 'wide',\n",
       " 'lawns',\n",
       " 'and',\n",
       " 'friendly',\n",
       " 'trees',\n",
       " ',',\n",
       " 'so',\n",
       " 'when',\n",
       " 'a',\n",
       " 'young',\n",
       " 'man',\n",
       " 'at',\n",
       " 'the',\n",
       " 'office',\n",
       " 'suggested',\n",
       " 'that',\n",
       " 'we',\n",
       " 'take',\n",
       " 'a',\n",
       " 'house',\n",
       " 'together',\n",
       " 'in',\n",
       " 'a',\n",
       " 'commuting',\n",
       " 'town',\n",
       " 'it',\n",
       " 'sounded',\n",
       " 'like',\n",
       " 'a',\n",
       " 'great',\n",
       " 'idea',\n",
       " '.',\n",
       " 'He',\n",
       " 'found',\n",
       " 'the',\n",
       " 'house',\n",
       " ',',\n",
       " 'a',\n",
       " 'weather',\n",
       " 'beaten',\n",
       " 'cardboard',\n",
       " 'bungalow',\n",
       " 'at',\n",
       " 'eighty',\n",
       " 'a',\n",
       " 'month',\n",
       " ',',\n",
       " 'but',\n",
       " 'at',\n",
       " 'the',\n",
       " 'last',\n",
       " 'minute',\n",
       " 'the',\n",
       " 'firm',\n",
       " 'ordered',\n",
       " 'him',\n",
       " 'to',\n",
       " 'Washington',\n",
       " 'and',\n",
       " 'I',\n",
       " 'went',\n",
       " '\\x18',\n",
       " 'out',\n",
       " 'to',\n",
       " 'the',\n",
       " 'country',\n",
       " 'alone',\n",
       " '.',\n",
       " 'I',\n",
       " 'had',\n",
       " 'a',\n",
       " 'dog',\n",
       " ',',\n",
       " 'at',\n",
       " 'least',\n",
       " 'I',\n",
       " 'had',\n",
       " 'him',\n",
       " 'for',\n",
       " 'a',\n",
       " 'few',\n",
       " 'days',\n",
       " 'until',\n",
       " 'he',\n",
       " 'ran',\n",
       " 'away',\n",
       " ',',\n",
       " 'and',\n",
       " 'an',\n",
       " 'old',\n",
       " 'Dodge',\n",
       " 'and',\n",
       " 'a',\n",
       " 'Finnish',\n",
       " 'woman',\n",
       " 'who',\n",
       " 'made',\n",
       " 'my',\n",
       " 'bed',\n",
       " 'and',\n",
       " 'cooked',\n",
       " 'breakfast',\n",
       " 'and',\n",
       " 'muttered',\n",
       " 'Finnish',\n",
       " 'wisdom',\n",
       " 'to',\n",
       " 'herself',\n",
       " 'over',\n",
       " 'the',\n",
       " 'electric',\n",
       " 'stove',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'lonely',\n",
       " 'for',\n",
       " ...]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab021e",
   "metadata": {},
   "source": [
    "### Deep Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "6e87c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_clean(text):\n",
    "    # remove utf8 encoding characters and some punctuations\n",
    "    text = [re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff\\xad\\x0c6§\\\\\\£\\Â*_<>\"\"⎫•{}Γ~]', ' ', word) for word in text]\n",
    "    text = [word for word in text if word.isalpha()]\n",
    "  \n",
    "    #removing all punctuation\n",
    "    re_punc = re.compile( ' [%s] ' % re.escape(string.punctuation))\n",
    "    text = [re_punc.sub( '' , word) for word in text]\n",
    "    \n",
    "    # replace whitespace characters with actual whitespace\n",
    "    text = [re.sub(r'\\s', ' ',word)for word in text]\n",
    "\n",
    "    # remove or standardize some recurring common and meaninless words/phrases/characters\n",
    "    text = [re.sub(r'\\s*â€\\s*', ' ', word)for word in text]\n",
    "\n",
    "    # replace odd quotation marks with a standard\n",
    "    text = [re.sub(r'[‘’“”]', \"'\", word)for word in text]\n",
    "\n",
    "\n",
    "    # unify some abbreviations\n",
    "    text = [re.sub(r'&', 'and', word)for word in text]\n",
    "    text = [re.sub(r'\\se\\.g\\.\\s', ' eg ', word)for word in text]\n",
    "    text = [re.sub(r'\\si\\.e\\.\\s', ' ie ', word)for word in text]\n",
    "    text = [re.sub('coroll\\.', 'coroll', word)for word in text]\n",
    "    text = [re.sub('pt\\.', 'pt', word)for word in text]\n",
    "\n",
    "    # remove roman numerals, first capitalized ones\n",
    "    text = [re.sub(r'\\s((I{2,}V*X*\\.*)|(IV\\.*)|(IX\\.*)|(V\\.*)|(V+I*\\.*)|(X+L*V*I*]\\.*))\\s', ' ', word) for word in text]\n",
    "    # then lowercase\n",
    "    text = [re.sub(r'\\s((i{2,}v*x*\\.*)|(iv\\.*)|(ix\\.*)|(v\\.*)|(v+i*\\.*)|(x+l*v*i*\\.*))\\s', ' ', word) for word in text]\n",
    "\n",
    "    # remove periods and commas flanked by numbers\n",
    "    text = [re.sub(r'\\d\\.\\d', ' ', word)for word in text]\n",
    "    text = [re.sub(r'\\d,\\d', ' ', word)for word in text]\n",
    "\n",
    "    # remove the number-letter-number pattern used for many citations\n",
    "    text = [re.sub(r'\\d*\\w{,2}\\d', ' ', word)for word in text]\n",
    "\n",
    "    # remove numerical characters\n",
    "    text = [re.sub(r'\\d+', ' ', word)for word in text]\n",
    "\n",
    "\n",
    "    # remove isolated colons and semicolons that result from removal of titles\n",
    "    text = [re.sub(r'\\s+:\\s*', ' ', word)for word in text]\n",
    "    text = [re.sub(r'\\s+;\\s*', ' ', word)for word in text]\n",
    "\n",
    "    # remove isolated letters (do it several times because strings of isolated letters do not get captured properly)\n",
    "    text = [re.sub(r'\\s[^aAI\\.]\\s', ' ', word)for word in text]\n",
    "    text = [re.sub(r'\\s[^aAI\\.]\\s', ' ', word)for word in text]\n",
    "    text = [re.sub(r'\\s[^aAI\\.]\\s', ' ', word)for word in text]\n",
    "    text = [re.sub(r'\\s[^aAI\\.]\\s', ' ', word)for word in text]\n",
    "    text = [re.sub(r'\\s[^aAI\\.]\\s', ' ', word)for word in text]\n",
    "    text = [re.sub(r'\\s[^aAI\\.]\\s', ' ', word)for word in text]\n",
    "    \n",
    "    # remove isolated letters at the end of sentences or before commas\n",
    "    text = [re.sub(r'\\s[^aI]\\.', '.',word)for word in text]\n",
    "    text = [re.sub(r'\\s[^aI],', ',', word)for word in text]\n",
    "\n",
    "    # deal with spaces around periods and commas\n",
    "    text = [re.sub(r'\\s+,\\s+', ', ', word)for word in text]\n",
    "    text = [re.sub(r'\\s+\\.\\s+', '. ', word)for word in text]\n",
    "\n",
    "    # remove empty parantheses\n",
    "    text = [re.sub(r'(\\(\\s*\\.*\\s*\\))|(\\(\\s*,*\\s*)\\)', ' ', word)for word in text]\n",
    "\n",
    "    # reduce multiple periods, commas, or whitespaces into a single one\n",
    "    text = [re.sub(r'\\.+', '.', word)for word in text]\n",
    "    text = [re.sub(r',+', ',', word)for word in text]\n",
    "    text = [re.sub(r'\\s+', ' ', word)for word in text]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "3660de86",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=baseline_clean(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "739a502d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Great',\n",
       " 'Gatsby',\n",
       " 'By',\n",
       " 'Scott',\n",
       " 'Fitzgerald',\n",
       " 'Then',\n",
       " 'wear',\n",
       " 'the',\n",
       " 'gold',\n",
       " 'hat',\n",
       " 'if',\n",
       " 'that',\n",
       " 'will',\n",
       " 'move',\n",
       " 'her',\n",
       " 'If',\n",
       " 'you',\n",
       " 'can',\n",
       " 'bounce',\n",
       " 'high',\n",
       " 'bounce',\n",
       " 'for',\n",
       " 'her',\n",
       " 'too',\n",
       " 'Till',\n",
       " 'she',\n",
       " 'cry',\n",
       " 'lover',\n",
       " 'I',\n",
       " 'must',\n",
       " 'have',\n",
       " 'you',\n",
       " 'THOMAS',\n",
       " 'PARKE',\n",
       " 'The',\n",
       " 'Great',\n",
       " 'Gatsby',\n",
       " 'Chapter',\n",
       " 'I',\n",
       " 'n',\n",
       " 'my',\n",
       " 'younger',\n",
       " 'and',\n",
       " 'more',\n",
       " 'vulnerable',\n",
       " 'years',\n",
       " 'my',\n",
       " 'father',\n",
       " 'gave',\n",
       " 'me',\n",
       " 'some',\n",
       " 'advice',\n",
       " 'that',\n",
       " 'been',\n",
       " 'turning',\n",
       " 'over',\n",
       " 'in',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'ever',\n",
       " 'since',\n",
       " 'you',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'criticizing',\n",
       " 'any',\n",
       " 'one',\n",
       " 'he',\n",
       " 'told',\n",
       " 'me',\n",
       " 'remember',\n",
       " 'that',\n",
       " 'all',\n",
       " 'the',\n",
       " 'people',\n",
       " 'in',\n",
       " 'this',\n",
       " 'world',\n",
       " 'had',\n",
       " 'the',\n",
       " 'advantages',\n",
       " 'that',\n",
       " 'He',\n",
       " 'say',\n",
       " 'any',\n",
       " 'more',\n",
       " 'but',\n",
       " 'always',\n",
       " 'been',\n",
       " 'unusually',\n",
       " 'communicative',\n",
       " 'in',\n",
       " 'a',\n",
       " 'reserved',\n",
       " 'way',\n",
       " 'and',\n",
       " 'I',\n",
       " 'understood',\n",
       " 'that',\n",
       " 'he',\n",
       " 'meant',\n",
       " 'a',\n",
       " 'great',\n",
       " 'deal',\n",
       " 'more',\n",
       " 'than',\n",
       " 'that',\n",
       " 'In',\n",
       " 'consequence',\n",
       " 'inclined',\n",
       " 'to',\n",
       " 'reserve',\n",
       " 'all',\n",
       " 'judgments',\n",
       " 'a',\n",
       " 'habit',\n",
       " 'that',\n",
       " 'has',\n",
       " 'opened',\n",
       " 'up',\n",
       " 'many',\n",
       " 'curious',\n",
       " 'natures',\n",
       " 'to',\n",
       " 'me',\n",
       " 'and',\n",
       " 'also',\n",
       " 'made',\n",
       " 'me',\n",
       " 'the',\n",
       " 'victim',\n",
       " 'of',\n",
       " 'not',\n",
       " 'a',\n",
       " 'few',\n",
       " 'veteran',\n",
       " 'bores',\n",
       " 'The',\n",
       " 'abnormal',\n",
       " 'mind',\n",
       " 'is',\n",
       " 'quick',\n",
       " 'to',\n",
       " 'detect',\n",
       " 'and',\n",
       " 'attach',\n",
       " 'itself',\n",
       " 'to',\n",
       " 'this',\n",
       " 'quality',\n",
       " 'when',\n",
       " 'it',\n",
       " 'appears',\n",
       " 'in',\n",
       " 'a',\n",
       " 'normal',\n",
       " 'person',\n",
       " 'and',\n",
       " 'so',\n",
       " 'it',\n",
       " 'came',\n",
       " 'about',\n",
       " 'that',\n",
       " 'in',\n",
       " 'college',\n",
       " 'I',\n",
       " 'was',\n",
       " 'unjustly',\n",
       " 'accused',\n",
       " 'of',\n",
       " 'being',\n",
       " 'a',\n",
       " 'politician',\n",
       " 'because',\n",
       " 'I',\n",
       " 'was',\n",
       " 'privy',\n",
       " 'to',\n",
       " 'the',\n",
       " 'secret',\n",
       " 'griefs',\n",
       " 'of',\n",
       " 'wild',\n",
       " 'unknown',\n",
       " 'men',\n",
       " 'Most',\n",
       " 'of',\n",
       " 'the',\n",
       " 'confidences',\n",
       " 'were',\n",
       " 'frequently',\n",
       " 'I',\n",
       " 'have',\n",
       " 'feigned',\n",
       " 'sleep',\n",
       " 'preoccupation',\n",
       " 'or',\n",
       " 'a',\n",
       " 'hostile',\n",
       " 'levity',\n",
       " 'when',\n",
       " 'I',\n",
       " 'realized',\n",
       " 'by',\n",
       " 'some',\n",
       " 'unmistakable',\n",
       " 'sign',\n",
       " 'that',\n",
       " 'an',\n",
       " 'intimate',\n",
       " 'revelation',\n",
       " 'was',\n",
       " 'quivering',\n",
       " 'on',\n",
       " 'the',\n",
       " 'for',\n",
       " 'the',\n",
       " 'intimate',\n",
       " 'revelations',\n",
       " 'of',\n",
       " 'young',\n",
       " 'men',\n",
       " 'or',\n",
       " 'at',\n",
       " 'least',\n",
       " 'the',\n",
       " 'terms',\n",
       " 'in',\n",
       " 'which',\n",
       " 'they',\n",
       " 'express',\n",
       " 'them',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'plagiaristic',\n",
       " 'and',\n",
       " 'marred',\n",
       " 'by',\n",
       " 'obvious',\n",
       " 'suppressions',\n",
       " 'Reserving',\n",
       " 'judgments',\n",
       " 'is',\n",
       " 'a',\n",
       " 'matter',\n",
       " 'of',\n",
       " 'infinite',\n",
       " 'hope',\n",
       " 'I',\n",
       " 'am',\n",
       " 'still',\n",
       " 'a',\n",
       " 'little',\n",
       " 'afraid',\n",
       " 'of',\n",
       " 'missing',\n",
       " 'something',\n",
       " 'if',\n",
       " 'I',\n",
       " 'forget',\n",
       " 'that',\n",
       " 'as',\n",
       " 'my',\n",
       " 'fa',\n",
       " 'ther',\n",
       " 'snobbishly',\n",
       " 'suggested',\n",
       " 'and',\n",
       " 'I',\n",
       " 'snobbishly',\n",
       " 'repeat',\n",
       " 'a',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'decencies',\n",
       " 'is',\n",
       " 'parcelled',\n",
       " 'out',\n",
       " 'unequally',\n",
       " 'at',\n",
       " 'birth',\n",
       " 'And',\n",
       " 'after',\n",
       " 'boasting',\n",
       " 'this',\n",
       " 'way',\n",
       " 'of',\n",
       " 'my',\n",
       " 'tolerance',\n",
       " 'I',\n",
       " 'come',\n",
       " 'to',\n",
       " 'the',\n",
       " 'admission',\n",
       " 'that',\n",
       " 'it',\n",
       " 'has',\n",
       " 'a',\n",
       " 'limit',\n",
       " 'Conduct',\n",
       " 'may',\n",
       " 'be',\n",
       " 'founded',\n",
       " 'on',\n",
       " 'the',\n",
       " 'hard',\n",
       " 'rock',\n",
       " 'or',\n",
       " 'the',\n",
       " 'wet',\n",
       " 'marshes',\n",
       " 'but',\n",
       " 'after',\n",
       " 'a',\n",
       " 'certain',\n",
       " 'point',\n",
       " 'I',\n",
       " 'care',\n",
       " 'what',\n",
       " 'founded',\n",
       " 'on',\n",
       " 'When',\n",
       " 'I',\n",
       " 'came',\n",
       " 'back',\n",
       " 'from',\n",
       " 'the',\n",
       " 'East',\n",
       " 'last',\n",
       " 'autumn',\n",
       " 'I',\n",
       " 'felt',\n",
       " 'that',\n",
       " 'I',\n",
       " 'wanted',\n",
       " 'the',\n",
       " 'world',\n",
       " 'to',\n",
       " 'be',\n",
       " 'in',\n",
       " 'uniform',\n",
       " 'and',\n",
       " 'at',\n",
       " 'a',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'moral',\n",
       " 'attention',\n",
       " 'forever',\n",
       " 'I',\n",
       " 'wanted',\n",
       " 'no',\n",
       " 'more',\n",
       " 'riotous',\n",
       " 'excursions',\n",
       " 'with',\n",
       " 'privileged',\n",
       " 'glimpses',\n",
       " 'into',\n",
       " 'the',\n",
       " 'human',\n",
       " 'heart',\n",
       " 'Only',\n",
       " 'Gatsby',\n",
       " 'the',\n",
       " 'man',\n",
       " 'who',\n",
       " 'gives',\n",
       " 'his',\n",
       " 'name',\n",
       " 'to',\n",
       " 'this',\n",
       " 'book',\n",
       " 'was',\n",
       " 'exempt',\n",
       " 'from',\n",
       " 'my',\n",
       " 'Gatsby',\n",
       " 'who',\n",
       " 'represented',\n",
       " 'everything',\n",
       " 'for',\n",
       " 'which',\n",
       " 'I',\n",
       " 'have',\n",
       " 'an',\n",
       " 'unaffected',\n",
       " 'scorn',\n",
       " 'If',\n",
       " 'personality',\n",
       " 'is',\n",
       " 'an',\n",
       " 'unbroken',\n",
       " 'series',\n",
       " 'of',\n",
       " 'successful',\n",
       " 'gestures',\n",
       " 'then',\n",
       " 'there',\n",
       " 'was',\n",
       " 'something',\n",
       " 'gorgeous',\n",
       " 'about',\n",
       " 'him',\n",
       " 'some',\n",
       " 'heightened',\n",
       " 'sensitivity',\n",
       " 'to',\n",
       " 'the',\n",
       " 'promises',\n",
       " 'of',\n",
       " 'life',\n",
       " 'as',\n",
       " 'if',\n",
       " 'he',\n",
       " 'were',\n",
       " 'related',\n",
       " 'to',\n",
       " 'one',\n",
       " 'of',\n",
       " 'those',\n",
       " 'intricate',\n",
       " 'machines',\n",
       " 'that',\n",
       " 'register',\n",
       " 'earthquakes',\n",
       " 'ten',\n",
       " 'thousand',\n",
       " 'miles',\n",
       " 'away',\n",
       " 'This',\n",
       " 'responsiveness',\n",
       " 'had',\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'that',\n",
       " 'flabby',\n",
       " 'impressionability',\n",
       " 'which',\n",
       " 'is',\n",
       " 'dignified',\n",
       " 'under',\n",
       " 'the',\n",
       " 'name',\n",
       " 'of',\n",
       " 'the',\n",
       " 'it',\n",
       " 'was',\n",
       " 'an',\n",
       " 'extraordinary',\n",
       " 'gift',\n",
       " 'for',\n",
       " 'hope',\n",
       " 'a',\n",
       " 'romantic',\n",
       " 'readiness',\n",
       " 'such',\n",
       " 'as',\n",
       " 'I',\n",
       " 'have',\n",
       " 'never',\n",
       " 'found',\n",
       " 'in',\n",
       " 'any',\n",
       " 'other',\n",
       " 'person',\n",
       " 'and',\n",
       " 'which',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'likely',\n",
       " 'I',\n",
       " 'shall',\n",
       " 'ever',\n",
       " 'find',\n",
       " 'again',\n",
       " 'Gatsby',\n",
       " 'turned',\n",
       " 'out',\n",
       " 'all',\n",
       " 'right',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " 'it',\n",
       " 'is',\n",
       " 'what',\n",
       " 'preyed',\n",
       " 'on',\n",
       " 'Gatsby',\n",
       " 'what',\n",
       " 'foul',\n",
       " 'dust',\n",
       " 'floated',\n",
       " 'in',\n",
       " 'the',\n",
       " 'wake',\n",
       " 'of',\n",
       " 'his',\n",
       " 'dreams',\n",
       " 'that',\n",
       " 'temporarily',\n",
       " 'closed',\n",
       " 'out',\n",
       " 'my',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'the',\n",
       " 'abortive',\n",
       " 'sorrows',\n",
       " 'and',\n",
       " 'shortwinded',\n",
       " 'elations',\n",
       " 'of',\n",
       " 'men',\n",
       " 'My',\n",
       " 'family',\n",
       " 'have',\n",
       " 'been',\n",
       " 'prominent',\n",
       " 'people',\n",
       " 'in',\n",
       " 'this',\n",
       " 'city',\n",
       " 'for',\n",
       " 'three',\n",
       " 'generations',\n",
       " 'The',\n",
       " 'raways',\n",
       " 'are',\n",
       " 'something',\n",
       " 'of',\n",
       " 'a',\n",
       " 'clan',\n",
       " 'and',\n",
       " 'we',\n",
       " 'have',\n",
       " 'a',\n",
       " 'tradition',\n",
       " 'that',\n",
       " 'descended',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Dukes',\n",
       " 'of',\n",
       " 'Buccleuch',\n",
       " 'but',\n",
       " 'the',\n",
       " 'actual',\n",
       " 'founder',\n",
       " 'of',\n",
       " 'my',\n",
       " 'line',\n",
       " 'was',\n",
       " 'my',\n",
       " 'brother',\n",
       " 'who',\n",
       " 'came',\n",
       " 'here',\n",
       " 'in',\n",
       " 'sent',\n",
       " 'a',\n",
       " 'substitute',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Civil',\n",
       " 'War',\n",
       " 'and',\n",
       " 'started',\n",
       " 'the',\n",
       " 'wholesale',\n",
       " 'hardware',\n",
       " 'business',\n",
       " 'that',\n",
       " 'my',\n",
       " 'father',\n",
       " 'carries',\n",
       " 'on',\n",
       " 'today',\n",
       " 'I',\n",
       " 'never',\n",
       " 'saw',\n",
       " 'this',\n",
       " 'but',\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'look',\n",
       " 'like',\n",
       " 'with',\n",
       " 'special',\n",
       " 'reference',\n",
       " 'to',\n",
       " 'the',\n",
       " 'rather',\n",
       " 'painting',\n",
       " 'that',\n",
       " 'hangs',\n",
       " 'in',\n",
       " 'office',\n",
       " 'I',\n",
       " 'graduated',\n",
       " 'from',\n",
       " 'New',\n",
       " 'Haven',\n",
       " 'in',\n",
       " 'just',\n",
       " 'a',\n",
       " 'quarter',\n",
       " 'of',\n",
       " 'a',\n",
       " 'century',\n",
       " 'after',\n",
       " 'my',\n",
       " 'father',\n",
       " 'and',\n",
       " 'a',\n",
       " 'little',\n",
       " 'later',\n",
       " 'I',\n",
       " 'participated',\n",
       " 'in',\n",
       " 'that',\n",
       " 'delayed',\n",
       " 'Teutonic',\n",
       " 'migration',\n",
       " 'known',\n",
       " 'as',\n",
       " 'the',\n",
       " 'Great',\n",
       " 'War',\n",
       " 'I',\n",
       " 'enjoyed',\n",
       " 'the',\n",
       " 'so',\n",
       " 'thoroughly',\n",
       " 'that',\n",
       " 'I',\n",
       " 'came',\n",
       " 'back',\n",
       " 'restless',\n",
       " 'Instead',\n",
       " 'of',\n",
       " 'being',\n",
       " 'the',\n",
       " 'warm',\n",
       " 'center',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'the',\n",
       " 'now',\n",
       " 'seemed',\n",
       " 'like',\n",
       " 'the',\n",
       " 'ragged',\n",
       " 'edge',\n",
       " 'of',\n",
       " 'the',\n",
       " 'so',\n",
       " 'I',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'go',\n",
       " 'east',\n",
       " 'and',\n",
       " 'learn',\n",
       " 'the',\n",
       " 'bond',\n",
       " 'business',\n",
       " 'Everybody',\n",
       " 'I',\n",
       " 'knew',\n",
       " 'was',\n",
       " 'in',\n",
       " 'the',\n",
       " 'bond',\n",
       " 'business',\n",
       " 'so',\n",
       " 'I',\n",
       " 'supposed',\n",
       " 'it',\n",
       " 'could',\n",
       " 'support',\n",
       " 'one',\n",
       " 'more',\n",
       " 'single',\n",
       " 'man',\n",
       " 'All',\n",
       " 'my',\n",
       " 'aunts',\n",
       " 'and',\n",
       " 'uncles',\n",
       " 'talked',\n",
       " 'it',\n",
       " 'over',\n",
       " 'as',\n",
       " 'if',\n",
       " 'they',\n",
       " 'were',\n",
       " 'choosing',\n",
       " 'a',\n",
       " 'for',\n",
       " 'me',\n",
       " 'and',\n",
       " 'finally',\n",
       " 'said',\n",
       " 'with',\n",
       " 'very',\n",
       " 'grave',\n",
       " 'hesitant',\n",
       " 'faces',\n",
       " 'Father',\n",
       " 'agreed',\n",
       " 'to',\n",
       " 'finance',\n",
       " 'me',\n",
       " 'for',\n",
       " 'a',\n",
       " 'year',\n",
       " 'and',\n",
       " 'after',\n",
       " 'various',\n",
       " 'delays',\n",
       " 'I',\n",
       " 'came',\n",
       " 'east',\n",
       " 'permanently',\n",
       " 'I',\n",
       " 'thought',\n",
       " 'in',\n",
       " 'the',\n",
       " 'spring',\n",
       " 'of',\n",
       " 'The',\n",
       " 'practical',\n",
       " 'thing',\n",
       " 'was',\n",
       " 'to',\n",
       " 'find',\n",
       " 'rooms',\n",
       " 'in',\n",
       " 'the',\n",
       " 'city',\n",
       " 'but',\n",
       " 'it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'warm',\n",
       " 'season',\n",
       " 'and',\n",
       " 'I',\n",
       " 'had',\n",
       " 'just',\n",
       " 'left',\n",
       " 'a',\n",
       " 'country',\n",
       " 'of',\n",
       " 'wide',\n",
       " 'lawns',\n",
       " 'and',\n",
       " 'friendly',\n",
       " 'trees',\n",
       " 'so',\n",
       " 'when',\n",
       " 'a',\n",
       " 'young',\n",
       " 'man',\n",
       " 'at',\n",
       " 'the',\n",
       " 'office',\n",
       " 'suggested',\n",
       " 'that',\n",
       " 'we',\n",
       " 'take',\n",
       " 'a',\n",
       " 'house',\n",
       " 'together',\n",
       " 'in',\n",
       " 'a',\n",
       " 'commuting',\n",
       " 'town',\n",
       " 'it',\n",
       " 'sounded',\n",
       " 'like',\n",
       " 'a',\n",
       " 'great',\n",
       " 'idea',\n",
       " 'He',\n",
       " 'found',\n",
       " 'the',\n",
       " 'house',\n",
       " 'a',\n",
       " 'weather',\n",
       " 'beaten',\n",
       " 'cardboard',\n",
       " 'bungalow',\n",
       " 'at',\n",
       " 'eighty',\n",
       " 'a',\n",
       " 'month',\n",
       " 'but',\n",
       " 'at',\n",
       " 'the',\n",
       " 'last',\n",
       " 'minute',\n",
       " 'the',\n",
       " 'firm',\n",
       " 'ordered',\n",
       " 'him',\n",
       " 'to',\n",
       " 'Washington',\n",
       " 'and',\n",
       " 'I',\n",
       " 'went',\n",
       " 'out',\n",
       " 'to',\n",
       " 'the',\n",
       " 'country',\n",
       " 'alone',\n",
       " 'I',\n",
       " 'had',\n",
       " 'a',\n",
       " 'dog',\n",
       " 'at',\n",
       " 'least',\n",
       " 'I',\n",
       " 'had',\n",
       " 'him',\n",
       " 'for',\n",
       " 'a',\n",
       " 'few',\n",
       " 'days',\n",
       " 'until',\n",
       " 'he',\n",
       " 'ran',\n",
       " 'away',\n",
       " 'and',\n",
       " 'an',\n",
       " 'old',\n",
       " 'Dodge',\n",
       " 'and',\n",
       " 'a',\n",
       " 'Finnish',\n",
       " 'woman',\n",
       " 'who',\n",
       " 'made',\n",
       " 'my',\n",
       " 'bed',\n",
       " 'and',\n",
       " 'cooked',\n",
       " 'breakfast',\n",
       " 'and',\n",
       " 'muttered',\n",
       " 'Finnish',\n",
       " 'wisdom',\n",
       " 'to',\n",
       " 'herself',\n",
       " 'over',\n",
       " 'the',\n",
       " 'electric',\n",
       " 'stove',\n",
       " 'It',\n",
       " 'was',\n",
       " 'lonely',\n",
       " 'for',\n",
       " 'a',\n",
       " 'day',\n",
       " 'or',\n",
       " 'so',\n",
       " 'until',\n",
       " 'one',\n",
       " 'morning',\n",
       " 'some',\n",
       " 'man',\n",
       " 'more',\n",
       " 'recently',\n",
       " 'arrived',\n",
       " 'than',\n",
       " 'I',\n",
       " 'stopped',\n",
       " 'me',\n",
       " 'on',\n",
       " 'the',\n",
       " 'road',\n",
       " 'do',\n",
       " 'you',\n",
       " 'get',\n",
       " 'to',\n",
       " 'West',\n",
       " 'Egg',\n",
       " 'village',\n",
       " 'he',\n",
       " 'asked',\n",
       " 'helplessly',\n",
       " 'I',\n",
       " 'told',\n",
       " 'him',\n",
       " 'And',\n",
       " 'as',\n",
       " 'I',\n",
       " 'walked',\n",
       " 'on',\n",
       " 'I',\n",
       " 'was',\n",
       " 'lonely',\n",
       " 'no',\n",
       " 'longer',\n",
       " 'I',\n",
       " 'was',\n",
       " 'a',\n",
       " 'guide',\n",
       " 'a',\n",
       " 'pathfinder',\n",
       " 'an',\n",
       " 'original',\n",
       " 'settler',\n",
       " 'He',\n",
       " 'had',\n",
       " 'casually',\n",
       " 'conferred',\n",
       " 'on',\n",
       " 'me',\n",
       " 'the',\n",
       " 'freedom',\n",
       " 'of',\n",
       " 'the',\n",
       " 'neighborhood',\n",
       " 'And',\n",
       " 'so',\n",
       " 'with',\n",
       " 'the',\n",
       " 'sunshine',\n",
       " 'and',\n",
       " 'the',\n",
       " 'great',\n",
       " 'bursts',\n",
       " 'of',\n",
       " 'leaves',\n",
       " 'growing',\n",
       " 'on',\n",
       " 'the',\n",
       " 'just',\n",
       " 'as',\n",
       " 'things',\n",
       " 'grow',\n",
       " 'in',\n",
       " 'fast',\n",
       " 'I',\n",
       " 'had',\n",
       " 'that',\n",
       " 'familiar',\n",
       " 'conviction',\n",
       " 'that',\n",
       " 'life',\n",
       " 'was',\n",
       " 'beginning',\n",
       " 'over',\n",
       " 'again',\n",
       " 'with',\n",
       " 'the',\n",
       " 'summer',\n",
       " 'There',\n",
       " 'was',\n",
       " 'so',\n",
       " 'much',\n",
       " 'to',\n",
       " 'read',\n",
       " 'for',\n",
       " 'one',\n",
       " 'thing',\n",
       " 'and',\n",
       " 'so',\n",
       " 'much',\n",
       " 'fine',\n",
       " 'health',\n",
       " 'to',\n",
       " 'be',\n",
       " 'pulled',\n",
       " 'down',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'young',\n",
       " 'air',\n",
       " 'I',\n",
       " 'bought',\n",
       " 'a',\n",
       " 'dozen',\n",
       " 'volumes',\n",
       " 'on',\n",
       " 'banking',\n",
       " 'and',\n",
       " 'credit',\n",
       " 'and',\n",
       " ...]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3911f0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "dc5ed8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total Sequences: 44710 \n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 5 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(txt)):\n",
    "# select sequence of tokens\n",
    "    seq = txt[i-length:i]\n",
    "# convert into a line\n",
    "    line = \" \".join(seq)\n",
    "# store\n",
    "    sequences.append(line)\n",
    "print( ' Total Sequences: %d ' % len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "155912d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Great Gatsby By Scott Fitzgerald'"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "1383d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "sequence_data = tokenizer.texts_to_sequences(sequences)\n",
    "\n",
    "sequence_data = np.array(sequence_data,dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "13f06eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sequence_data[:, :-1], sequence_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "a515f121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 213, 32, 50, 5548], dtype=object), 2555)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "2824ede7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5549\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "53d690ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [[1 213 32 50 5548]\n",
      " [213 32 50 5548 2555]\n",
      " [32 50 5548 2555 44]\n",
      " [50 5548 2555 44 1293]\n",
      " [5548 2555 44 1293 1]]\n",
      "The responses are:  [2555 44 1293 1 619]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "b0b9d7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "7477bd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = X.shape[1]\n",
    "sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6314bb",
   "metadata": {},
   "source": [
    "### Creating the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "d3a76bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=sequence_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "553c1c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, 5, 10)             55490     \n",
      "                                                                 \n",
      " lstm_22 (LSTM)              (None, 5, 100)            44400     \n",
      "                                                                 \n",
      " lstm_23 (LSTM)              (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 5549)              560449    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 750,839\n",
      "Trainable params: 750,839\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55636e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "18f0569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "1ccbdb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "e6e31a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.asarray(X).astype('float32')\n",
    "y=np.asarray(y).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "58593781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "350/350 [==============================] - 71s 79ms/step - loss: 6.7920\n",
      "Epoch 2/100\n",
      "350/350 [==============================] - 22s 62ms/step - loss: 6.4546\n",
      "Epoch 3/100\n",
      "350/350 [==============================] - 15s 43ms/step - loss: 6.4301\n",
      "Epoch 4/100\n",
      "350/350 [==============================] - 13s 38ms/step - loss: 6.3457\n",
      "Epoch 5/100\n",
      "350/350 [==============================] - 15s 42ms/step - loss: 6.2486\n",
      "Epoch 6/100\n",
      "350/350 [==============================] - 12s 34ms/step - loss: 6.0890\n",
      "Epoch 7/100\n",
      "350/350 [==============================] - 12s 33ms/step - loss: 5.9195\n",
      "Epoch 8/100\n",
      "350/350 [==============================] - 14s 41ms/step - loss: 5.8053\n",
      "Epoch 9/100\n",
      "350/350 [==============================] - 13s 38ms/step - loss: 5.7021\n",
      "Epoch 10/100\n",
      "350/350 [==============================] - 12s 35ms/step - loss: 5.5951\n",
      "Epoch 11/100\n",
      "350/350 [==============================] - 14s 40ms/step - loss: 5.4943\n",
      "Epoch 12/100\n",
      "350/350 [==============================] - 15s 42ms/step - loss: 5.4001\n",
      "Epoch 13/100\n",
      "350/350 [==============================] - 11s 32ms/step - loss: 5.3169\n",
      "Epoch 14/100\n",
      "350/350 [==============================] - 11s 31ms/step - loss: 5.2352\n",
      "Epoch 15/100\n",
      "350/350 [==============================] - 11s 30ms/step - loss: 5.1535\n",
      "Epoch 16/100\n",
      "350/350 [==============================] - 10s 30ms/step - loss: 5.0723\n",
      "Epoch 17/100\n",
      "350/350 [==============================] - 11s 30ms/step - loss: 4.9900\n",
      "Epoch 18/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 4.9096\n",
      "Epoch 19/100\n",
      "350/350 [==============================] - 10s 30ms/step - loss: 4.8300\n",
      "Epoch 20/100\n",
      "350/350 [==============================] - 10s 30ms/step - loss: 4.7458\n",
      "Epoch 21/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 4.6591\n",
      "Epoch 22/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 4.5721\n",
      "Epoch 23/100\n",
      "350/350 [==============================] - 11s 30ms/step - loss: 4.4824\n",
      "Epoch 24/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 4.3918\n",
      "Epoch 25/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 4.3029\n",
      "Epoch 26/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 4.2141\n",
      "Epoch 27/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 4.1284\n",
      "Epoch 28/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 4.0423\n",
      "Epoch 29/100\n",
      "350/350 [==============================] - 10s 30ms/step - loss: 3.9612\n",
      "Epoch 30/100\n",
      "350/350 [==============================] - 10s 30ms/step - loss: 3.8818\n",
      "Epoch 31/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 3.8095\n",
      "Epoch 32/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 3.7322\n",
      "Epoch 33/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 3.6610\n",
      "Epoch 34/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 3.5941\n",
      "Epoch 35/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 3.5230\n",
      "Epoch 36/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 3.4674\n",
      "Epoch 37/100\n",
      "350/350 [==============================] - 10s 30ms/step - loss: 3.4059\n",
      "Epoch 38/100\n",
      "350/350 [==============================] - 11s 31ms/step - loss: 3.3488\n",
      "Epoch 39/100\n",
      "350/350 [==============================] - 10s 30ms/step - loss: 3.2928\n",
      "Epoch 40/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 3.2411\n",
      "Epoch 41/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 3.1886\n",
      "Epoch 42/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 3.1398\n",
      "Epoch 43/100\n",
      "350/350 [==============================] - 10s 30ms/step - loss: 3.0951\n",
      "Epoch 44/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 3.0467\n",
      "Epoch 45/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 3.0088\n",
      "Epoch 46/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.9597\n",
      "Epoch 47/100\n",
      "350/350 [==============================] - 11s 30ms/step - loss: 2.9250\n",
      "Epoch 48/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.8824\n",
      "Epoch 49/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.8408\n",
      "Epoch 50/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.8055\n",
      "Epoch 51/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.7643\n",
      "Epoch 52/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.7294\n",
      "Epoch 53/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.6948\n",
      "Epoch 54/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.6628\n",
      "Epoch 55/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.6237\n",
      "Epoch 56/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.5919\n",
      "Epoch 57/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.5596\n",
      "Epoch 58/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.5255\n",
      "Epoch 59/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.4980\n",
      "Epoch 60/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.4592\n",
      "Epoch 61/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.4322\n",
      "Epoch 62/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.4036\n",
      "Epoch 63/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.3770\n",
      "Epoch 64/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.3473\n",
      "Epoch 65/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.3148\n",
      "Epoch 66/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.2940\n",
      "Epoch 67/100\n",
      "350/350 [==============================] - 10s 30ms/step - loss: 2.2608\n",
      "Epoch 68/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.2357\n",
      "Epoch 69/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.2099\n",
      "Epoch 70/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.1817\n",
      "Epoch 71/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.1576\n",
      "Epoch 72/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.1333\n",
      "Epoch 73/100\n",
      "350/350 [==============================] - 10s 30ms/step - loss: 2.1101\n",
      "Epoch 74/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.0810\n",
      "Epoch 75/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.0526\n",
      "Epoch 76/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.0297\n",
      "Epoch 77/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 2.0129\n",
      "Epoch 78/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.9876\n",
      "Epoch 79/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.9640\n",
      "Epoch 80/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.9424\n",
      "Epoch 81/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.9220\n",
      "Epoch 82/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.8961\n",
      "Epoch 83/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.8789\n",
      "Epoch 84/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.8473\n",
      "Epoch 85/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.8304\n",
      "Epoch 86/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.8155\n",
      "Epoch 87/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.7917\n",
      "Epoch 88/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.7695\n",
      "Epoch 89/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.7488\n",
      "Epoch 90/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.7332\n",
      "Epoch 91/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.7141\n",
      "Epoch 92/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.6940\n",
      "Epoch 93/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.6736\n",
      "Epoch 94/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.6589\n",
      "Epoch 95/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.6374\n",
      "Epoch 96/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.6199\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 10s 29ms/step - loss: 1.6016\n",
      "Epoch 98/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.5871\n",
      "Epoch 99/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.5569\n",
      "Epoch 100/100\n",
      "350/350 [==============================] - 10s 29ms/step - loss: 1.5511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16fec2c2be0>"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y,batch_size=128, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "c4645d90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1\n",
      "Test accuracy: .\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X, y, verbose=0)\n",
    "string_score=str(score)\n",
    "print('Test loss:', string_score[0])\n",
    "print('Test accuracy:', string_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7423dd",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "ebb0d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the model and tokenizer\n",
    "\n",
    "model = load_model('nextword1.h5')\n",
    "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb'))\n",
    "\n",
    "def Predict_Next_Words(model, tokenizer, germ_text):\n",
    "    \"\"\"\n",
    "        In this function we are using the tokenizer and models trained\n",
    "        and we are creating the sequence of the text entered and then\n",
    "        using our model to predict and return the the predicted word.\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in range(1):\n",
    "        encoded = tokenizer.texts_to_sequences([germ_text])[0]\n",
    "        padded_sequences=pad_sequences([encoded], sequence_length, truncating=\"pre\")\n",
    "        \n",
    "        print(encoded, padded_sequences)\n",
    "        for i in (model.predict(padded_sequences)[0]).argsort()[-5:][::-1]:\n",
    "          predicted_word = tokenizer.index_word[i]\n",
    "          print(\"Next word suggestion:\",predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "4f46e2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inclined to reserve all judgments a'"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "germ_text = sequences[110]\n",
    "germ_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "82944def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2963, 5, 1955, 32, 1956, 3] [[   5 1955   32 1956    3]]\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='embedding_3_input'), name='embedding_3_input', description=\"created by layer 'embedding_3_input'\"), but it was called on an input with incompatible shape (None, 5).\n",
      "Next word suggestion: afternoon\n",
      "Next word suggestion: are\n",
      "Next word suggestion: never\n",
      "Next word suggestion: him\n",
      "Next word suggestion: is\n"
     ]
    }
   ],
   "source": [
    "Predict_Next_Words(model, tokenizer,  germ_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f130ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "e808d49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your line: inclined to reserve all judgments a\n",
      "[2963, 5, 1955, 32, 1956, 3]\n",
      "[2963, 5, 1955, 32, 1956, 3] [[   5 1955   32 1956    3]]\n",
      "Next word suggestion: afternoon\n",
      "Next word suggestion: are\n",
      "Next word suggestion: never\n",
      "Next word suggestion: him\n",
      "Next word suggestion: is\n",
      "Enter your line: stop the script\n",
      "Ending The Program.....\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    We are testing our model and we will run the model\n",
    "    until the user decides to stop the script.\n",
    "    While the script is running we try and check if \n",
    "    the prediction can be made on the text. If no\n",
    "    prediction can be made we just continue.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# text1 = \"At nine o’clock\"\n",
    "# text2 = \"He was balancing\"\n",
    "# text3 = \"I’m going to tell \"\n",
    "# text4 = \"Most of the confidences are\"\n",
    "# text5 = \"stop the script\"\n",
    "\n",
    "\n",
    "\n",
    "while(True):\n",
    "\n",
    "    text = input(\"Enter your line: \")\n",
    "    \n",
    "    \n",
    "    if text == \"stop the script\":\n",
    "        print(\"Ending The Program.....\")\n",
    "        break\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            text = text.strip().lower()\n",
    "            encoded_text = tokenizer.texts_to_sequences([text])[0]\n",
    "            print(encoded_text)\n",
    "            pad_encoded = pad_sequences([encoded_text], sequence_length, truncating='pre')\n",
    "            print(encoded_text, pad_encoded)\n",
    "            for i in (model.predict(pad_encoded)[0]).argsort()[-5:][::-1]:\n",
    "                predicted_word = tokenizer.index_word[i]\n",
    "                print(\"Next word suggestion:\",predicted_word)\n",
    "                \n",
    "                \n",
    "            \n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfcb447",
   "metadata": {},
   "source": [
    "## Run the following code in terminal to view the streamlit app attached with my submission\n",
    "### streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b059993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
